{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Youtuber's Name: asmr glow\n",
      "asmr glow\n",
      "How far back in time do you want to go? (YYYY-MM-DD) or (all-time): all-time\n",
      "all-time\n",
      "https://www.youtube.com/results?search_query=asmr+glow\n",
      "https://www.youtube.com/channel/UCFmL725KKPx2URVPvH3Gp8w/videos\n",
      "https://www.youtube.com/channel/UCFmL725KKPx2URVPvH3Gp8w/about\n",
      "Convert function date is not valid.\n",
      "Artist: ASMR Glow\n",
      "Subscribers: 638447\n",
      "Views: 103678517\n",
      "Joined: None\n",
      "https://www.youtube.com/playlist?list=UUFmL725KKPx2URVPvH3Gp8w\n",
      "https://www.youtube.com/watch?v=oFhoGsWTbZg&list=UUFmL725KKPx2URVPvH3Gp8w\n",
      "Total videos: 144\n",
      "Proceed with scrape? (y/n)y\n",
      "Getting urls...\n",
      "0.6944444444444444% complete...\n",
      "Convert function date is not valid.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'datetime.datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-441533eb7ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Break if Date Less than Input Date Range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpublish_date_convert\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconverted_input_date\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'datetime.datetime'"
     ]
    }
   ],
   "source": [
    "# To do:\n",
    "# Fix Dates, must be another published on vs premiered on discrepancy -- OK\n",
    "# Fix Titles -- OK\n",
    "# Looks like missing one (1) video for Gibi 344/345 -- OK\n",
    "# DUPLICATES -- OK\n",
    "# EDGE CASES ▶ -- OK\n",
    "# User entered date range -- OK\n",
    "# Limit Date Range to Years and then YTD, e.g. 2016, 2017, YTD, All-Time\n",
    "# Better commenting -- \n",
    "# Duration --\n",
    "# Flask -- \n",
    "# HTML/CSS/JS -- \n",
    "# Dev Database -- OK\n",
    "# Wrong Date bug -- \n",
    "\n",
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Get Scrape Date\n",
    "scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "scrape_datetime = datetime.today()\n",
    "#scrape_date = datetime.now().strftime(\"%m/%d/%Y\")\n",
    "#scrape_date_convert = datetime.date(scrape_date, '%Y-%m-%d')\n",
    "\n",
    "# Connect to Database\n",
    "connection = create_engine('mysql://root:Mars@127.0.0.1')\n",
    "\n",
    "# Convert Date from Jan 1, 1999 format to datetime object\n",
    "converted_date = \"\"\n",
    "raw_months = {\"Jan\": 1, \"Feb\": 2, \"Mar\" : 3, \"Apr\" : 4, \n",
    "              \"May\" : 5, \"Jun\" : 6, \"Jul\" : 7, \"Aug\" : 8,\n",
    "              \"Sep\" : 9, \"Oct\" : 10, \"Nov\" : 11, \"Dec\" : 12}\n",
    "\n",
    "def convertDate(raw_date):\n",
    "    \n",
    "    try:\n",
    "        converted_date = \"\"\n",
    "        number_month = raw_months.get(raw_date[0])\n",
    "        date_str = (str(number_month) + \"/\" + raw_date[1] + \"/\" + raw_date[2]).replace(\",\", \"\")\n",
    "        converted_date = datetime.date(date_str, '%m/%d/%Y')\n",
    "        return converted_date\n",
    "    \n",
    "    except:\n",
    "        print(\"Convert function date is not valid.\")\n",
    "\n",
    "# Get Youtuber's Name\n",
    "input_name = input(\"Enter Youtuber's Name: \")\n",
    "print(input_name)\n",
    "\n",
    "input_date_range = input(\"How far back in time do you want to go? (YYYY-MM-DD) or (all-time): \")\n",
    "print(input_date_range)\n",
    "\n",
    "if input_date_range == \"all-time\":\n",
    "    converted_input_date = datetime.strptime(\"1950-01-01\", '%Y-%m-%d')\n",
    "    \n",
    "else:\n",
    "    try:\n",
    "        converted_input_date = datetime.strptime(input_date_range, '%Y-%m-%d')\n",
    "        \n",
    "    except:\n",
    "        print(\"Input date is not valid.\")\n",
    "        exit()\n",
    "\n",
    "list_name = input_name.split()\n",
    "\n",
    "converted_name = input_name\n",
    "\n",
    "if len(list_name) > 1:\n",
    "    \n",
    "    converted_name = \"\"\n",
    "    \n",
    "    for i in range(len(list_name)):\n",
    "\n",
    "        converted_name = converted_name + list_name[i]\n",
    "        \n",
    "        if i != len(list_name)-1:\n",
    "            converted_name = converted_name + \"+\"\n",
    "\n",
    "search_name = converted_name\n",
    "start_url = \"https://www.youtube.com/results?search_query=\" + search_name\n",
    "\n",
    "print(start_url)\n",
    "\n",
    "get_youtube_url_response = requests.get(start_url)\n",
    "youtube_name_soup = bs(get_youtube_url_response.text, \"lxml\")\n",
    "raw_youtube_name_link = youtube_name_soup.find_all(\"div\", class_=\"yt-lockup-byline\")[0].a.get(\"href\")\n",
    "videos_link = \"https://www.youtube.com\" + raw_youtube_name_link + \"/videos\"\n",
    "about_link = \"https://www.youtube.com\" + raw_youtube_name_link + \"/about\"\n",
    "\n",
    "print(videos_link)\n",
    "print(about_link)\n",
    "\n",
    "# Get About Information\n",
    "about_html = requests.get(about_link)\n",
    "\n",
    "# Parse HTML\n",
    "about_soup = bs(about_html.text, \"lxml\")\n",
    "\n",
    "# Artist Information\n",
    "artist_name = about_soup.find(\"meta\", property=\"og:title\").get(\"content\")\n",
    "\n",
    "subscribers = about_soup.find_all(\"span\", class_=\"about-stat\")[0].text\n",
    "subscribers_int = int(subscribers.split(\" \")[0].replace(\",\",\"\"))\n",
    "\n",
    "total_views = about_soup.find_all(\"span\", class_=\"about-stat\")[1].text\n",
    "total_views_int = int(total_views[3:len(total_views)].split(\" \")[0].replace(\",\",\"\"))\n",
    "\n",
    "joined = about_soup.find_all(\"span\", class_=\"about-stat\")[2].text\n",
    "joined_temp = joined.split(\" \")[1:4]\n",
    "joined_convert = convertDate(joined_temp)\n",
    "\n",
    "print(f\"Artist: {artist_name}\")\n",
    "print(f\"Subscribers: {subscribers_int}\")\n",
    "print(f\"Views: {total_views_int}\")\n",
    "print(f\"Joined: {joined_convert}\")\n",
    "\n",
    "# Convert User Name to UU Format\n",
    "youtube_code = raw_youtube_name_link.split(\"/\")[2]\n",
    "\n",
    "if youtube_code[0:2] == \"UC\":\n",
    "    \n",
    "    youtube_code = raw_youtube_name_link.split(\"/\")[2]\n",
    "    playlist_link = \"https://www.youtube.com\" + \"/playlist?list=UU\" + youtube_code[2:] \n",
    "\n",
    "elif youtube_code[0:2] != \"UC\":\n",
    "    \n",
    "    youtube_code_raw = about_soup.find(\"link\", rel=\"canonical\").get(\"href\")\n",
    "    youtube_code = youtube_code_raw.split(\"/\")[4]\n",
    "    playlist_link = \"https://www.youtube.com\" + \"/playlist?list=UU\" + youtube_code[2:]  \n",
    "    \n",
    "print(playlist_link)\n",
    "\n",
    "# Get Playlist Response\n",
    "playlist_response = requests.get(playlist_link)\n",
    "\n",
    "# Create Playlist Soup Object\n",
    "playlist_soup = bs(playlist_response.text, 'lxml')\n",
    "\n",
    "# Get First Video URL as Starting Point\n",
    "first_video = \"https://www.youtube.com\" + playlist_soup.find_all(\"a\", class_=\"pl-video-title-link\")[0].get(\"href\").split(\"&\")[0]\n",
    "first_video_within_playlist = first_video + \"&\" + playlist_link.split(\"?\")[1]\n",
    "\n",
    "print(first_video_within_playlist)\n",
    "\n",
    "# Create Soup Object for First Video Inside Playlist\n",
    "playlist_inside_request = requests.get(first_video_within_playlist) \n",
    "\n",
    "playlist_inside_soup = bs(playlist_inside_request.text, \"lxml\")\n",
    "\n",
    "urls_all = []\n",
    "total_videos_in_playlist = int(playlist_inside_soup.find(\"span\", id=\"playlist-length\").text.replace(\" videos\",\"\").replace(\",\",\"\"))\n",
    "print(f\"Total videos: {total_videos_in_playlist}\")\n",
    "number_of_videos_in_page = len(playlist_inside_soup.find_all(\"span\", class_=\"index\")) \n",
    "last_video_index = int(playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].text.replace(\"\\n        \",\"\").replace(\"\\n    \",\"\"))\n",
    "last_shown_link = playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].find_next(\"a\").get(\"href\")\n",
    "link_fix = \"https://www.youtube.com\" + last_shown_link\n",
    "\n",
    "proceed = input(\"Proceed with scrape? (y/n)\")\n",
    "\n",
    "if proceed == \"n\" or proceed == \"N\":\n",
    "    exit()\n",
    "\n",
    "print(\"Getting urls...\")\n",
    "\n",
    "for i in range(total_videos_in_playlist):   \n",
    "\n",
    "    if i == 0:       \n",
    "        first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "        url = \"https://www.youtube.com\" + first_link.find_next(\"a\").get(\"href\")\n",
    "        original_url = url.split(\"&\")[0]\n",
    "        urls_all.append(original_url)\n",
    "        next_link = first_link\n",
    "        \n",
    "    elif i == last_video_index:       \n",
    "        playlist_inside_request = requests.get(link_fix)\n",
    "        playlist_inside_soup = bs(playlist_inside_request.text, \"lxml\")\n",
    "        last_shown_link = playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].find_next(\"a\").get(\"href\")\n",
    "        link_fix = \"https://www.youtube.com\" + last_shown_link\n",
    "        last_video_index = int(playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].text.replace(\"\\n        \",\"\").replace(\"\\n    \",\"\"))\n",
    "        first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i+1}\\n    \")\n",
    "        \n",
    "        if first_link is None:           \n",
    "            next_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "            \n",
    "        else:          \n",
    "            next_link = first_link\n",
    "        \n",
    "        next_url = \"https://www.youtube.com\" + next_link.find_next(\"a\").get(\"href\")\n",
    "        original_url = next_url.split(\"&\")[0]\n",
    "        urls_all.append(original_url)\n",
    "        number_of_videos_in_page = len(playlist_inside_soup.find_all(\"span\", class_=\"index\")) - 1\n",
    "              \n",
    "    else:\n",
    " \n",
    "        if i == 1:\n",
    "            first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "\n",
    "        elif playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i}\\n    \") is None:\n",
    "            first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "        \n",
    "        else:\n",
    "            first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i}\\n    \")\n",
    "\n",
    "        next_link = first_link\n",
    "        next_link = next_link.find_next(\"span\", class_=\"index\")\n",
    "        next_url = \"https://www.youtube.com\" + next_link.find_next(\"a\").get(\"href\")\n",
    "        original_url = next_url.split(\"&\")[0]\n",
    "        urls_all.append(original_url)\n",
    "\n",
    "# Going to Each Video and Extracting Data\n",
    "published_on = []\n",
    "raw_published_on = []\n",
    "views = []\n",
    "date = []\n",
    "duration_videos = []\n",
    "likes = []\n",
    "dislikes = []\n",
    "title_videos = []\n",
    "categories = []\n",
    "paid_list = []\n",
    "family_friendly = []\n",
    "\n",
    "for i in range(len(urls_all)):\n",
    "    \n",
    "    video_url = urls_all[i]\n",
    "    video_response = requests.get(video_url)\n",
    "    video_soup = bs(video_response.text, 'lxml')\n",
    "    \n",
    "    # Publish Date\n",
    "    raw_publish_date = video_soup.find(\"div\", id=\"watch-uploader-info\").text\n",
    "    raw_published_on.append(raw_publish_date)\n",
    "    \n",
    "    # Handle All Raw Dates \"Premiered\", \"\"Published\", \"Streamed\", \"X Hours Ago\"\n",
    "    publish_date_format = raw_publish_date.split(\" \")[len(raw_publish_date.split(\" \"))-3:len(raw_publish_date.split(\" \"))]\n",
    "    \n",
    "    if publish_date_format[1] == \"hours\":\n",
    "        publish_date_convert = scrape_datetime\n",
    "        \n",
    "    else:\n",
    "        publish_date_convert = convertDate(publish_date_format)\n",
    "    \n",
    "    # Break if Date Less than Input Date Range\n",
    "    if publish_date_convert < converted_input_date:\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        published_on.append(publish_date_convert)\n",
    "    \n",
    "    # Title\n",
    "    title = video_soup.find(\"title\").text.replace(\" - YouTube\", \"\")\n",
    "    title_videos.append(title)\n",
    "    \n",
    "    # Views\n",
    "    string_views = video_soup.find(\"div\", id=\"watch7-views-info\").text.replace(\" views\", \"\").replace(\",\",\"\").replace(\"\\n\",\"\")\n",
    "    int_views = int(string_views)\n",
    "    views.append(int_views)\n",
    "    \n",
    "    #Duration\n",
    "    duration = video_soup.find(\"meta\", itemprop=\"duration\").get(\"content\").replace(\"PT\",\"\").split(\"M\")\n",
    "    duration_mins = int(video_soup.find(\"meta\", itemprop=\"duration\").get(\"content\").replace(\"PT\",\"\").split(\"M\")[0])\n",
    "    duration_secs = int(duration[1].replace(\"S\",\"\"))\n",
    "    total_duration = duration_mins + duration_secs/60\n",
    "    duration_videos.append(total_duration)\n",
    "    \n",
    "    # Likes\n",
    "    string_likes = video_soup.find(\"button\", title=\"I like this\").text\n",
    "    if string_likes != \"\":\n",
    "        int_likes = int(string_likes.replace(\",\",\"\"))\n",
    "        likes.append(int_likes)\n",
    "    else:\n",
    "        likes.append(0)\n",
    "    \n",
    "    # Dislikes\n",
    "    string_dislikes = video_soup.find(\"button\", title=\"I dislike this\").text\n",
    "    if string_dislikes != \"\":\n",
    "        int_dislikes = int(string_dislikes.replace(\",\",\"\"))\n",
    "        dislikes.append(int_dislikes)\n",
    "    else:\n",
    "        dislikes.append(0)\n",
    "    \n",
    "    # Category\n",
    "    category = video_soup.find(\"h4\", class_=\"title\", text=\"\\n      Category\\n    \").find_next(\"a\").text\n",
    "    categories.append(category)\n",
    "    \n",
    "    # Paid\n",
    "    paid = video_soup.find(\"meta\", itemprop=\"paid\").get(\"content\")\n",
    "    paid_list.append(paid)\n",
    "    \n",
    "    # Family Friendly\n",
    "    family = video_soup.find(\"meta\", itemprop=\"isFamilyFriendly\").get(\"content\")\n",
    "    family_friendly.append(family)\n",
    "    \n",
    "    percent_complete = ((i+1) / (len(urls_all)))*100\n",
    "    \n",
    "    print(f\"{percent_complete}% complete...\")\n",
    "    \n",
    "urls_to_date = urls_all[0:len(published_on)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"Artist\" : artist_name,\n",
    "                   \"Scrape_Date\" : scrape_date,\n",
    "                   \"Joined\" : joined_convert,\n",
    "                   \"Subscribers\" : subscribers_int,\n",
    "                   \"Total_Views\" : total_views_int,\n",
    "                   \"Published\": published_on,\n",
    "                   \"Title\" : title_videos,\n",
    "                   \"Category\" : categories,\n",
    "                   \"Duration\" : duration_videos,\n",
    "                   \"Views\" : views,\n",
    "                   \"Likes\" : likes,\n",
    "                   \"Dislikes\" : dislikes,\n",
    "                   \"Paid\" : paid_list,\n",
    "                   \"Family_Friendly\" : family_friendly,\n",
    "                   \"URL\" : urls_to_date,\n",
    "                  })\n",
    "\n",
    "df = df.sort_values(\"Published\",ascending=False)\n",
    "\n",
    "#df = df.set_index(\"Artist\")\n",
    "\n",
    "# Saving to CSV\n",
    "df.to_csv(f\"{artist_name}_scrape.csv\")\n",
    "\n",
    "# Saving to JSON\n",
    "df.to_json(f\"../js/{artist_name}_data.js\", orient=\"records\")\n",
    "\n",
    "# Insert Data into Database\n",
    "connection.execute(\"USE web_app_dev\")\n",
    "    \n",
    "for i in range(len(df)):\n",
    "    scrape_date = df.loc[i,\"Scrape_Date\"]\n",
    "    artist = df.loc[i,\"Artist\"]\n",
    "    joined = df.loc[i,\"Joined\"]\n",
    "    subscribers = df.loc[i,\"Subscribers\"]\n",
    "    total_views = df.loc[i,\"Total_Views\"]\n",
    "    published = df.loc[i,\"Published\"]\n",
    "    title = df.loc[i,\"Title\"]\n",
    "    category = df.loc[i,\"Category\"]\n",
    "    duration = df.loc[i,\"Duration\"]\n",
    "    views = df.loc[i,\"Views\"]\n",
    "    likes = df.loc[i,\"Likes\"]\n",
    "    dislikes = df.loc[i,\"Dislikes\"]\n",
    "    paid = df.loc[i,\"Paid\"]\n",
    "    family_friendly = df.loc[i,\"Family_Friendly\"]\n",
    "    url =  df.loc[i,\"URL\"]\n",
    "    \n",
    "    connection.execute(f\"INSERT INTO video_data \\\n",
    "    (SCRAPE_DATE, ARTIST, PUBLISHED, TITLE, CATEGORY , DURATION,\\\n",
    "    VIEWS, LIKES, DISLIKES, PAID, FAMILY_FRIENDLY, URL)\\\n",
    "    VALUES ('{scrape_date}', '{artist}', '{published}', '{title}', '{category}',\\\n",
    "    '{duration}', '{views}', '{likes}', '{dislikes}', '{paid}',\\\n",
    "    '{family_friendly}', '{url}')\")\n",
    "    \n",
    "connection.execute(f\"INSERT INTO artist_info \\\n",
    "(SCRAPE_DATE, ARTIST, JOINED, SUBSCRIBERS, TOTAL_VIEWS)\\\n",
    "VALUES ('{scrape_date}', '{artist}', '{joined}', '{subscribers}',\\\n",
    "'{total_views}')\")\n",
    "\n",
    "print(\"Inserted data into database successfully...\")\n",
    "\n",
    "# View Data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDate(raw_date):\n",
    "    \n",
    "    try:\n",
    "        converted_date = \"\"\n",
    "        number_month = raw_months.get(raw_date[0])\n",
    "        date_str = (str(number_month) + \"/\" + raw_date[1] + \"/\" + raw_date[2]).replace(\",\", \"\")\n",
    "        converted_date = datetime.date(date_str, '%m/%d/%Y')\n",
    "        return converted_date\n",
    "    \n",
    "    except:\n",
    "        print(\"Convert function date is not valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_date = datetime.now().strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = scrape_date.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert function date is not valid.\n"
     ]
    }
   ],
   "source": [
    "convertDate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_months = {\"Jan\": 1, \"Feb\": 2, \"Mar\" : 3, \"Apr\" : 4, \n",
    "              \"May\" : 5, \"Jun\" : 6, \"Jul\" : 7, \"Aug\" : 8,\n",
    "              \"Sep\" : 9, \"Oct\" : 10, \"Nov\" : 11, \"Dec\" : 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 17, 1, 1, 33, 567511)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
