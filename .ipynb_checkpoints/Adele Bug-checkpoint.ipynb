{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template\n",
    "from flask import url_for\n",
    "from flask import request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import time\n",
    "from numpy import random\n",
    "pymysql.install_as_MySQLdb()\n",
    "global_search_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/anaconda3/envs/PythonData/lib/python3.6/site-packages/pymysql/cursors.py:170: Warning: (1007, \"Can't create database 'web_app_dev'; database exists\")\n",
      "  result = self._query(query)\n",
      "/Users/Ali/anaconda3/envs/PythonData/lib/python3.6/site-packages/pymysql/cursors.py:170: Warning: (1050, \"Table 'requests' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/channel/UCsRM0YB_dabtEPGPTKo-gcw/videos\n",
      "https://www.youtube.com/channel/UCsRM0YB_dabtEPGPTKo-gcw/about\n",
      "Artist: Adele\n",
      "Subscribers: 18655136\n",
      "Views: 8045115891\n",
      "Joined: 2008-04-04 00:00:00\n",
      "Found 3 playlists:\n",
      "Music: https://www.youtube.com/playlist?list=PLBrbKtPlBbNdQITGHHesg1Wxw6AOdJiB2\n",
      "More from the artist: https://www.youtube.com/playlist?list=PLBrbKtPlBbNdYprRwHu6hwpUWkg9F4tO1\n",
      "Uploads: https://www.youtube.com/playlist?list=UUsRM0YB_dabtEPGPTKo-gcw\n",
      "Getting Music urls now...\n",
      "Total videos: 36\n",
      "Getting More from the artist urls now...\n",
      "Total videos: 7\n",
      "Getting Uploads urls now...\n",
      "Total videos: 13\n",
      "2.0% complete...\n",
      "4.0% complete...\n",
      "5.0% complete...\n",
      "7.0% complete...\n",
      "9.0% complete...\n",
      "11.0% complete...\n",
      "12.0% complete...\n",
      "14.0% complete...\n",
      "16.0% complete...\n",
      "18.0% complete...\n",
      "20.0% complete...\n",
      "21.0% complete...\n",
      "23.0% complete...\n",
      "25.0% complete...\n",
      "27.0% complete...\n",
      "29.0% complete...\n",
      "30.0% complete...\n",
      "32.0% complete...\n",
      "34.0% complete...\n",
      "36.0% complete...\n",
      "38.0% complete...\n",
      "39.0% complete...\n",
      "41.0% complete...\n",
      "43.0% complete...\n",
      "45.0% complete...\n",
      "46.0% complete...\n",
      "48.0% complete...\n",
      "50.0% complete...\n",
      "52.0% complete...\n",
      "54.0% complete...\n",
      "55.0% complete...\n",
      "57.0% complete...\n",
      "59.0% complete...\n",
      "61.0% complete...\n",
      "62.0% complete...\n",
      "64.0% complete...\n",
      "66.0% complete...\n",
      "68.0% complete...\n",
      "70.0% complete...\n",
      "71.0% complete...\n",
      "73.0% complete...\n",
      "75.0% complete...\n",
      "77.0% complete...\n",
      "79.0% complete...\n",
      "80.0% complete...\n",
      "82.0% complete...\n",
      "84.0% complete...\n",
      "86.0% complete...\n",
      "88.0% complete...\n",
      "89.0% complete...\n",
      "91.0% complete...\n",
      "93.0% complete...\n",
      "95.0% complete...\n",
      "96.0% complete...\n",
      "98.0% complete...\n",
      "100.0% complete...\n",
      "Inserting data into database...\n",
      "Inserted data into database successfully...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/anaconda3/envs/PythonData/lib/python3.6/site-packages/pymysql/cursors.py:170: Warning: (1050, \"Table 'artists' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "# Start Clock, Set Variables\n",
    "start = time.time()\n",
    "json_data = []\n",
    "json_data_1 = []\n",
    "cache = \"\"\n",
    "percent_complete_str = \"0\"\n",
    "\n",
    "# Connect to Database Server\n",
    "connection = create_engine('mysql://root:Mars@127.0.0.1')\n",
    "\n",
    "# Creating database if not exists\n",
    "connection.execute(\"CREATE DATABASE IF NOT EXISTS web_app_dev\")\n",
    "connection.execute(\"USE web_app_dev\")\n",
    "\n",
    "# Creating Table for Requests\n",
    "connection.execute(\"\\\n",
    "CREATE TABLE IF NOT EXISTS requests(\\\n",
    "ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\\\n",
    "SCRAPE_DATE DATETIME,\\\n",
    "SEARCH_NAME VARCHAR(355) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST_CODE VARCHAR(255) CHARACTER SET UTF8MB4 NOT NULL\\\n",
    ")\")\n",
    "\n",
    "# Convert Date from Jan 1, 1999 format to datetime object\n",
    "raw_months = {\"Jan\": 1, \"Feb\": 2, \"Mar\" : 3, \"Apr\" : 4, \n",
    "            \"May\" : 5, \"Jun\" : 6, \"Jul\" : 7, \"Aug\" : 8,\n",
    "            \"Sep\" : 9, \"Oct\" : 10, \"Nov\" : 11, \"Dec\" : 12}\n",
    "\n",
    "def convertDate(raw_date):\n",
    "\n",
    "    try:\n",
    "        converted_date = \"\"\n",
    "        number_month = raw_months.get(raw_date[0])\n",
    "        date_str = (str(number_month) + \"/\" + raw_date[1] + \"/\" + raw_date[2]).replace(\",\", \"\")\n",
    "        converted_date = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "        return converted_date\n",
    "\n",
    "    except:\n",
    "        print(f\"{raw_date} Convert function date is not valid.\")\n",
    "\n",
    "converted_input_date = datetime.strptime(\"1944-06-06\", '%Y-%m-%d')\n",
    "\n",
    "# Get Search Key Value\n",
    "# name_key = request.args['name']\n",
    "input_name = \"UCsRM0YB_dabtEPGPTKo-gcw\"\n",
    "youtube_code = input_name\n",
    "\n",
    "#table_name = youtube_code_orig.replace(\"-\",\"\")\n",
    "search_name = input_name.replace(\"-\",\"_replaced_\")\n",
    "\n",
    "# Get Scrape Date\n",
    "scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "scrape_datetime = datetime.utcnow()\n",
    "\n",
    "\n",
    "# try:\n",
    "# First Links\n",
    "videos_link = \"https://www.youtube.com/channel/\" + youtube_code + \"/videos\"\n",
    "about_link = \"https://www.youtube.com/channel/\" + youtube_code + \"/about\"\n",
    "\n",
    "print(videos_link)\n",
    "print(about_link)\n",
    "\n",
    "# Get About Information\n",
    "about_html = requests.get(about_link)\n",
    "\n",
    "# Parse HTML\n",
    "about_soup = bs(about_html.text, \"lxml\")\n",
    "\n",
    "# Artist Image\n",
    "artist_image = about_soup.find(\"img\", class_=\"channel-header-profile-image\").get(\"src\")\n",
    "\n",
    "# Artist Information\n",
    "try:\n",
    "    artist_name = about_soup.find(\"meta\", property=\"og:title\").get(\"content\")\n",
    "\n",
    "    subscribers = about_soup.find_all(\"span\", class_=\"about-stat\")[0].text\n",
    "    subscribers_str = subscribers.split(\" \")[0]\n",
    "\n",
    "    try:\n",
    "        subscribers_int = int(subscribers.split(\" \")[0].replace(\",\",\"\"))\n",
    "\n",
    "    except:\n",
    "        subscribers_int = \"Not available\"\n",
    "\n",
    "    total_views = about_soup.find_all(\"span\", class_=\"about-stat\")[1].text\n",
    "    total_views_str = total_views[3:len(total_views)].split(\" \")[0]\n",
    "\n",
    "    try:\n",
    "        total_views_int = int(total_views[3:len(total_views)].split(\" \")[0].replace(\",\",\"\"))\n",
    "        joined = about_soup.find_all(\"span\", class_=\"about-stat\")[2].text\n",
    "        joined_temp = joined.split(\" \")[1:4]\n",
    "        joined_convert = convertDate(joined_temp)\n",
    "        joined_str = str(joined_convert).split(\" \")[0]\n",
    "\n",
    "    except:\n",
    "        total_views = about_soup.find_all(\"span\", class_=\"about-stat\")[0].text\n",
    "        total_views_str = total_views[3:len(total_views)].split(\" \")[0]\n",
    "        total_views_int = int(total_views[3:len(total_views)].split(\" \")[0].replace(\",\",\"\"))\n",
    "\n",
    "        joined = about_soup.find_all(\"span\", class_=\"about-stat\")[1].text\n",
    "        joined_temp = joined.split(\" \")[1:4]\n",
    "        joined_convert = convertDate(joined_temp)\n",
    "        joined_str = str(joined_convert).split(\" \")[0]\n",
    "\n",
    "    print(f\"Artist: {artist_name}\")\n",
    "    print(f\"Subscribers: {subscribers_int}\")\n",
    "    print(f\"Views: {total_views_int}\")\n",
    "    print(f\"Joined: {joined_convert}\")\n",
    "\n",
    "except:\n",
    "    print(\"Something went wrong getting artist information..\")\n",
    "\n",
    "# Setting Table Name\n",
    "artist_db_name = youtube_code.replace(\"-\",\"_replaced_\")\n",
    "\n",
    "# Youtube Code\n",
    "youtube_code = input_name\n",
    "\n",
    "# Getting ALL Playlist Names\n",
    "videos_response=requests.get(videos_link)\n",
    "videos_soup = bs(videos_response.text,\"lxml\")\n",
    "videos_soup.find_all(\"span\",class_=\"branded-page-module-title-text\")\n",
    "\n",
    "playlist_names_html = videos_soup.find_all(\"span\",class_=\"branded-page-module-title-text\")\n",
    "playlist_names = []\n",
    "\n",
    "for name in playlist_names_html:\n",
    "\n",
    "    if name.text != \"\\nUploads\\n\":\n",
    "        playlist_names.append(name.text.replace(\"\\n\",\"\"))\n",
    "        \n",
    "playlist_names.append(\"Uploads\")\n",
    "\n",
    "# Getting ALL Playlist URLS\n",
    "playlist_urls_html = videos_soup.find_all(\"a\",class_=\"branded-page-module-title-link\")\n",
    "playlist_urls = []\n",
    "playlist_uploads_link = \"https://www.youtube.com\" + \"/playlist?list=UU\" + youtube_code[2:]\n",
    "\n",
    "for playlist in playlist_urls_html:\n",
    "\n",
    "    if \"/user/\" not in playlist.get(\"href\"):\n",
    "        playlist_urls.append(\"https://www.youtube.com\" + playlist.get(\"href\"))\n",
    "        \n",
    "playlist_urls.append(playlist_uploads_link)\n",
    "\n",
    "print(f\"Found {len(playlist_urls)} playlists:\")\n",
    "\n",
    "for i in range(len(playlist_urls)):\n",
    "    print(f\"{playlist_names[i]}: {playlist_urls[i]}\")\n",
    "\n",
    "urls_all = []\n",
    "counter = 0\n",
    "total_videos_all = 0\n",
    "for playlist_link in playlist_urls:    \n",
    "    \n",
    "    print(f\"Getting {playlist_names[counter]} urls now...\")\n",
    "\n",
    "    # Get Playlist Response\n",
    "    playlist_response = requests.get(playlist_link)\n",
    "\n",
    "    # Create Playlist Soup Object\n",
    "    playlist_soup = bs(playlist_response.text, 'lxml')\n",
    "\n",
    "    # Get First Video URL as Starting Point\n",
    "    first_video = \"https://www.youtube.com\" + playlist_soup.find_all(\"a\", class_=\"pl-video-title-link\")[0].get(\"href\").split(\"&\")[0]\n",
    "    first_video_within_playlist = first_video + \"&\" + playlist_link.split(\"?\")[1]\n",
    "\n",
    "#     print(first_video_within_playlist)\n",
    "\n",
    "    # Create Soup Object for First Video Inside Playlist\n",
    "    playlist_inside_request = requests.get(first_video_within_playlist) \n",
    "\n",
    "    playlist_inside_soup = bs(playlist_inside_request.text, \"lxml\")\n",
    "\n",
    "    \n",
    "    total_videos_in_playlist = int(playlist_inside_soup.find(\"span\", id=\"playlist-length\").text.replace(\" videos\",\"\").replace(\",\",\"\"))\n",
    "    print(f\"Total videos: {total_videos_in_playlist}\")\n",
    "    total_videos_all = total_videos_all + total_videos_in_playlist\n",
    "    \n",
    "    number_of_videos_in_page = len(playlist_inside_soup.find_all(\"span\", class_=\"index\")) \n",
    "    last_video_index = int(playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].text.replace(\"\\n        \",\"\").replace(\"\\n    \",\"\"))\n",
    "    last_shown_link = playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].find_next(\"a\").get(\"href\")\n",
    "    link_fix = \"https://www.youtube.com\" + last_shown_link\n",
    "\n",
    "#     print(\"Getting urls...\")\n",
    "\n",
    "    for i in range(total_videos_in_playlist):   \n",
    "\n",
    "        if i == 0:       \n",
    "            first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "            url = \"https://www.youtube.com\" + first_link.find_next(\"a\").get(\"href\")\n",
    "            original_url = url.split(\"&\")[0]\n",
    "            urls_all.append(original_url)\n",
    "            next_link = first_link\n",
    "\n",
    "        elif i == last_video_index:       \n",
    "            playlist_inside_request = requests.get(link_fix)\n",
    "            playlist_inside_soup = bs(playlist_inside_request.text, \"lxml\")\n",
    "            last_shown_link = playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].find_next(\"a\").get(\"href\")\n",
    "            link_fix = \"https://www.youtube.com\" + last_shown_link\n",
    "            last_video_index = int(playlist_inside_soup.find_all(\"span\", class_=\"index\")[-1].text.replace(\"\\n        \",\"\").replace(\"\\n    \",\"\"))\n",
    "            first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i+1}\\n    \")\n",
    "\n",
    "            if first_link is None:           \n",
    "                next_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "\n",
    "            else:          \n",
    "                next_link = first_link\n",
    "\n",
    "            next_url = \"https://www.youtube.com\" + next_link.find_next(\"a\").get(\"href\")\n",
    "            original_url = next_url.split(\"&\")[0]\n",
    "            urls_all.append(original_url)\n",
    "            number_of_videos_in_page = len(playlist_inside_soup.find_all(\"span\", class_=\"index\")) - 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            if i == 1:\n",
    "                first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "\n",
    "            elif playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i}\\n    \") is None:\n",
    "                first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        ▶\\n    \")\n",
    "\n",
    "            else:\n",
    "                first_link = playlist_inside_soup.find(\"span\", class_=\"index\", text=f\"\\n        {i}\\n    \")\n",
    "\n",
    "            next_link = first_link\n",
    "            next_link = next_link.find_next(\"span\", class_=\"index\")\n",
    "            next_url = \"https://www.youtube.com\" + next_link.find_next(\"a\").get(\"href\")\n",
    "            original_url = next_url.split(\"&\")[0]\n",
    "            urls_all.append(original_url)\n",
    "            \n",
    "    counter += 1\n",
    "\n",
    "#             request_duration = time.time() - start\n",
    "#         if request_duration >  1000:\n",
    "#             json_data = []\n",
    "#             reason = \"The request took too long to complete.\"\n",
    "#             return render_template(\"uh-oh.html\", data = json_data, reason=reason)\n",
    "\n",
    "# except:\n",
    "#     print(\"Something went wrong getting video urls..\")\n",
    "\n",
    "# Going to Each Video and Extracting Data\n",
    "published_on = []\n",
    "published_on_str = []\n",
    "raw_published_on = []\n",
    "views = []\n",
    "date = []\n",
    "duration_videos = []\n",
    "likes = []\n",
    "dislikes = []\n",
    "title_videos = []\n",
    "categories = []\n",
    "paid_list = []\n",
    "family_friendly = []\n",
    "bump = 0\n",
    "\n",
    "print(f\"There are {len(urls_all)} total videos to get...\"\")\n",
    "\n",
    "for i in range(len(urls_all)):\n",
    "    try:\n",
    "        video_url = urls_all[i]\n",
    "        video_response = requests.get(video_url)\n",
    "        video_soup = bs(video_response.text, 'lxml')\n",
    "\n",
    "        # Publish Date\n",
    "        raw_publish_date = video_soup.find(\"div\", id=\"watch-uploader-info\").text\n",
    "        raw_published_on.append(raw_publish_date)\n",
    "\n",
    "        # Handle All Raw Dates \"Premiered\", \"\"Published\", \"Streamed\", \"X Hours Ago\"\n",
    "        publish_date_format = raw_publish_date.split(\" \")[len(raw_publish_date.split(\" \"))-3:len(raw_publish_date.split(\" \"))]\n",
    "\n",
    "        if publish_date_format[1] == \"hours\":\n",
    "            publish_date_convert = datetime.strptime(scrape_date, '%Y-%m-%d')\n",
    "\n",
    "        else:\n",
    "            publish_date_convert = convertDate(publish_date_format)\n",
    "\n",
    "        # Break if Date Less than Input Date Range\n",
    "        if publish_date_convert < converted_input_date:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            published_on.append(publish_date_convert)\n",
    "            published_on_str.append(str(publish_date_convert).split(\" \")[0])\n",
    "\n",
    "        # Title\n",
    "        title = video_soup.find(\"title\").text.replace(\" - YouTube\", \"\")\n",
    "        title_videos.append(title)\n",
    "\n",
    "        # Views\n",
    "        string_views = video_soup.find(\"div\", id=\"watch7-views-info\").text.replace(\" views\", \"\").replace(\",\",\"\").replace(\"\\n\",\"\")\n",
    "        int_views = int(string_views)\n",
    "        views.append(int_views)\n",
    "\n",
    "        #Duration\n",
    "        duration = video_soup.find(\"meta\", itemprop=\"duration\").get(\"content\").replace(\"PT\",\"\").split(\"M\")\n",
    "        duration_mins = int(video_soup.find(\"meta\", itemprop=\"duration\").get(\"content\").replace(\"PT\",\"\").split(\"M\")[0])\n",
    "        duration_secs = int(duration[1].replace(\"S\",\"\"))\n",
    "        total_duration = round(duration_mins + duration_secs/60,2)\n",
    "        duration_videos.append(total_duration)\n",
    "\n",
    "        # Likes\n",
    "        string_likes = video_soup.find(\"button\", title=\"I like this\").text\n",
    "        if string_likes != \"\":\n",
    "            int_likes = int(string_likes.replace(\",\",\"\"))\n",
    "            likes.append(int_likes)\n",
    "        else:\n",
    "            likes.append(0)\n",
    "\n",
    "        # Dislikes\n",
    "        string_dislikes = video_soup.find(\"button\", title=\"I dislike this\").text\n",
    "        if string_dislikes != \"\":\n",
    "            int_dislikes = int(string_dislikes.replace(\",\",\"\"))\n",
    "            dislikes.append(int_dislikes)\n",
    "        else:\n",
    "            dislikes.append(0)\n",
    "\n",
    "        # Category\n",
    "        category = video_soup.find(\"h4\", class_=\"title\", text=\"\\n      Category\\n    \").find_next(\"a\").text\n",
    "        categories.append(category)\n",
    "\n",
    "        # Paid\n",
    "        paid = video_soup.find(\"meta\", itemprop=\"paid\").get(\"content\")\n",
    "        paid_list.append(paid)\n",
    "\n",
    "        # Family Friendly\n",
    "        family = video_soup.find(\"meta\", itemprop=\"isFamilyFriendly\").get(\"content\")\n",
    "        family_friendly.append(family)\n",
    "        \n",
    "        # Percent Complete\n",
    "        percent_complete = round(((i+1) / (len(urls_all)))*100,0)\n",
    "\n",
    "        percent_complete_str = str(percent_complete)\n",
    "\n",
    "        print(f\"{percent_complete}% complete...\")\n",
    "\n",
    "#         request_duration = time.time() - start\n",
    "#         if request_duration >  1000:\n",
    "#             json_data = []\n",
    "#             reason = \"The request took too long to complete.\"\n",
    "#             return render_template(\"uh-oh.html\", data = json_data, reason=reason)\n",
    "\n",
    "    # Remove any data apended to lists during an exception, account for smaller list size after removal vs. i\n",
    "    except:\n",
    "        print(f\"Skipped {video_url}...\")\n",
    "        try:\n",
    "            published_on.pop(i-bump)\n",
    "        except:\n",
    "            pass            \n",
    "        try:\n",
    "            raw_published_on.pop(i-bump)\n",
    "        except:\n",
    "            pass            \n",
    "        try:\n",
    "            views.pop(i-bump)\n",
    "        except:\n",
    "            pass            \n",
    "        try:\n",
    "            date.pop(i-bump)\n",
    "        except:\n",
    "            pass            \n",
    "        try:\n",
    "            duration_videos.pop(i-bump)\n",
    "        except:\n",
    "            pass            \n",
    "        try:\n",
    "            likes.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            dislikes.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            title_videos.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            categories.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            paid_list.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:    \n",
    "            family_friendly.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            urls_all.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            published_on_str.pop(i-bump)\n",
    "        except:\n",
    "            pass\n",
    "        bump = bump + 1\n",
    "        continue\n",
    "\n",
    "urls_to_date = urls_all[0:len(published_on)]\n",
    "\n",
    "youtube_code = input_name.replace(\"-\",\"_replaced_\")\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"ARTIST\" : artist_name,\n",
    "                \"SCRAPE_DATE\" : scrape_datetime,\n",
    "                \"SEARCH_NAME\" : global_search_name,\n",
    "                \"TOTAL_VIDEOS\" : total_videos_all,\n",
    "                \"JOINED\" : joined_convert,\n",
    "                \"SUBSCRIBERS\" : subscribers_int,\n",
    "                \"TOTAL_VIEWS\" : total_views_int,\n",
    "                \"PUBLISHED\": published_on,\n",
    "                \"PUBLISHED_STR\" : published_on_str,\n",
    "                \"TITLE\" : title_videos,\n",
    "                \"CATEGORY\" : categories,\n",
    "                \"DURATION\" : duration_videos,\n",
    "                \"VIEWS\" : views,\n",
    "                \"LIKES\" : likes,\n",
    "                \"DISLIKES\" : dislikes,\n",
    "                \"PAID\" : paid_list,\n",
    "                \"FAMILY_FRIENDLY\" : family_friendly,\n",
    "                \"URL\" : urls_to_date,\n",
    "                \"ARTIST_IMAGE\": artist_image,\n",
    "                \"ARTIST_CODE\" : youtube_code,\n",
    "                })\n",
    "\n",
    "df = df.sort_values(\"PUBLISHED\",ascending=False)\n",
    "\n",
    "# Saving to CSV\n",
    "#df.to_csv(f\"{artist_name}_scrape.csv\")\n",
    "\n",
    "# Saving to JSON\n",
    "json_data = df.to_json(orient=\"records\")\n",
    "\n",
    "# Insert Data into Database\n",
    "print(\"Inserting data into database...\")\n",
    "# Creating table for videos\n",
    "connection.execute(f\"\\\n",
    "CREATE TABLE IF NOT EXISTS {youtube_code} (\\\n",
    "ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\\\n",
    "SCRAPE_DATE DATETIME,\\\n",
    "SEARCH_NAME VARCHAR(355) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "PUBLISHED DATE,\\\n",
    "PUBLISHED_STR VARCHAR(255),\\\n",
    "TITLE VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "CATEGORY VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "DURATION FLOAT,\\\n",
    "VIEWS BIGINT,\\\n",
    "LIKES INT,\\\n",
    "DISLIKES INT,\\\n",
    "COMMENTS INT,\\\n",
    "PAID VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "FAMILY_FRIENDLY VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "URL VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST_CODE VARCHAR(255) CHARACTER SET UTF8MB4 NOT NULL\\\n",
    ")\")\n",
    "\n",
    "# Creating Table for Artist data\n",
    "connection.execute(\"\\\n",
    "CREATE TABLE IF NOT EXISTS artists(\\\n",
    "ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\\\n",
    "SCRAPE_DATE DATETIME,\\\n",
    "SEARCH_NAME VARCHAR(355) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "TOTAL_VIDEOS INT,\\\n",
    "JOINED DATE,\\\n",
    "SUBSCRIBERS INT,\\\n",
    "TOTAL_VIEWS BIGINT,\\\n",
    "ARTIST_IMAGE VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST_CODE VARCHAR(255) CHARACTER SET UTF8MB4 NOT NULL UNIQUE\\\n",
    ")\")\n",
    "\n",
    "# Creating Table for Requests\n",
    "connection.execute(\"\\\n",
    "CREATE TABLE IF NOT EXISTS requests(\\\n",
    "ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\\\n",
    "SCRAPE_DATE DATETIME,\\\n",
    "SEARCH_NAME VARCHAR(355) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST VARCHAR(255) CHARACTER SET UTF8MB4,\\\n",
    "ARTIST_CODE VARCHAR(255) CHARACTER SET UTF8MB4 NOT NULL\\\n",
    ")\")\n",
    "\n",
    "# Getting df values and inserting into appropriate tables\n",
    "for i in range(len(df)):\n",
    "    scrape_date = df.loc[i,\"SCRAPE_DATE\"]\n",
    "    search_name = df.loc[i,\"SEARCH_NAME\"]\n",
    "    table_name = artist_db_name\n",
    "    artist = df.loc[i,\"ARTIST\"].replace(\"`\",\"\")\n",
    "    joined = df.loc[i,\"JOINED\"]\n",
    "    subscribers = df.loc[i,\"SUBSCRIBERS\"]\n",
    "    total_views = df.loc[i,\"TOTAL_VIEWS\"]\n",
    "    published = df.loc[i,\"PUBLISHED\"]\n",
    "    published_str = df.loc[i,\"PUBLISHED_STR\"]\n",
    "    title = df.loc[i,\"TITLE\"].replace(\"'\",\"\").replace('\"',\"\").replace(']',\"\")\\\n",
    "    .replace('[',\"\").replace('\\\\',\"\").replace(\"%\",\"\").replace(\"`\",\"\")\n",
    "    category = df.loc[i,\"CATEGORY\"]\n",
    "    duration = df.loc[i,\"DURATION\"]\n",
    "    views = df.loc[i,\"VIEWS\"]\n",
    "    likes = df.loc[i,\"LIKES\"]\n",
    "    dislikes = df.loc[i,\"DISLIKES\"]\n",
    "    paid = df.loc[i,\"PAID\"]\n",
    "    family_friendly = df.loc[i,\"FAMILY_FRIENDLY\"]\n",
    "    url =  df.loc[i,\"URL\"]\n",
    "    artist_code =  df.loc[i,\"ARTIST_CODE\"]\n",
    "\n",
    "    connection.execute(f\"INSERT INTO {youtube_code}\\\n",
    "    (SCRAPE_DATE, SEARCH_NAME, ARTIST, PUBLISHED, PUBLISHED_STR, TITLE, CATEGORY , DURATION,\\\n",
    "    VIEWS, LIKES, DISLIKES, PAID, FAMILY_FRIENDLY, URL, ARTIST_CODE)\\\n",
    "    VALUES ('{scrape_date}','{search_name}', '{artist}', '{published}', \\\n",
    "    '{published_str}','{title}', '{category}',\\\n",
    "    '{duration}', '{views}', '{likes}', '{dislikes}', '{paid}',\\\n",
    "    '{family_friendly}', '{url}','{artist_code}')\")\n",
    "\n",
    "if subscribers == \"Not available\":\n",
    "    connection.execute(f\"INSERT INTO artists \\\n",
    "    (SCRAPE_DATE, SEARCH_NAME, ARTIST, TOTAL_VIDEOS, JOINED, SUBSCRIBERS, TOTAL_VIEWS, ARTIST_IMAGE, ARTIST_CODE)\\\n",
    "    VALUES ('{scrape_date}','{search_name}', '{artist}', '{total_videos_all}', \\\n",
    "    '{joined}', NULL,\\\n",
    "    '{total_views}','{artist_image}', '{artist_code}')\")\n",
    "\n",
    "    connection.execute(f\"INSERT INTO requests \\\n",
    "    (SCRAPE_DATE, SEARCH_NAME, ARTIST, ARTIST_CODE)\\\n",
    "    VALUES ('{scrape_date}','{search_name}', '{artist}','{artist_code}')\")\n",
    "\n",
    "else:\n",
    "    connection.execute(f\"INSERT INTO artists \\\n",
    "    (SCRAPE_DATE, SEARCH_NAME, ARTIST, TOTAL_VIDEOS, JOINED, SUBSCRIBERS, TOTAL_VIEWS, ARTIST_IMAGE, ARTIST_CODE)\\\n",
    "    VALUES ('{scrape_date}','{search_name}', '{artist}', '{total_videos_all}',\\\n",
    "    '{joined}', '{subscribers}', '{total_views}','{artist_image}','{artist_code}')\")\n",
    "\n",
    "    connection.execute(f\"INSERT INTO requests \\\n",
    "    (SCRAPE_DATE, SEARCH_NAME, ARTIST, ARTIST_CODE)\\\n",
    "    VALUES ('{scrape_date}','{search_name}', '{artist}','{artist_code}')\")\n",
    "\n",
    "\n",
    "print(\"Inserted data into database successfully...\")\n",
    "\n",
    "# Searching for input Artist\n",
    "df_cache = pd.read_sql(f\"SELECT artists.ARTIST, artists.SCRAPE_DATE, artists.SEARCH_NAME, JOINED, SUBSCRIBERS, TOTAL_VIEWS, \\\n",
    "{artist_db_name}.PUBLISHED_STR, artists.TOTAL_VIDEOS, artists.ARTIST_CODE, \\\n",
    "{artist_db_name}.TITLE, {artist_db_name}.CATEGORY , {artist_db_name}.DURATION, {artist_db_name}.VIEWS, \\\n",
    "{artist_db_name}.LIKES, {artist_db_name}.DISLIKES, {artist_db_name}.PAID, {artist_db_name}.FAMILY_FRIENDLY, \\\n",
    "{artist_db_name}.URL, artists.ARTIST_IMAGE FROM artists \\\n",
    "INNER JOIN {artist_db_name} \\\n",
    "ON artists.ARTIST_CODE = {artist_db_name}.ARTIST_CODE\", connection)\n",
    "\n",
    "# Artist 0 Information\n",
    "scrape_date = df_cache.loc[0,\"SCRAPE_DATE\"]\n",
    "cache = f\"{scrape_date} scrape\"\n",
    "json_data = df_cache.to_json(orient=\"records\")\n",
    "scrape_date = df_cache.loc[0,\"SCRAPE_DATE\"]\n",
    "scrape_date_str = str(scrape_date).split(\" \")[0]\n",
    "total_videos_str = format(df_cache.loc[1,\"TOTAL_VIDEOS\"],\",\") + \" Videos\"\n",
    "number_scraped = int(len(df_cache))\n",
    "artist_name = df_cache.loc[0,\"ARTIST\"]\n",
    "analytics_base_url = \"/query?name=\" + artist_db_name + \"&analytics=base\"\n",
    "subscribers_str = format(df_cache.loc[0,\"SUBSCRIBERS\"],\",\")\n",
    "joined_str = df_cache.loc[0,\"JOINED\"]\n",
    "total_views_str = format(df_cache.loc[0,\"TOTAL_VIEWS\"],\",\")\n",
    "artist_image = df_cache.loc[0,\"ARTIST_IMAGE\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
